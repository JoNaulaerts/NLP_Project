#!/usr/bin/env python3
# ML Learning Assistant - COMPLETE END-TO-END TEST SUITE
# ========================================================
# Tests EVERYTHING: Infrastructure, MCP, RAG, Web Search, Agents, Crews, Memory, UI

import os
import sys
import json
import time
import chromadb
from pathlib import Path
from datetime import datetime

# Color codes for terminal output
class Colors:
    GREEN = "\033[92m"
    RED = "\033[91m"
    YELLOW = "\033[93m"
    BLUE = "\033[94m"
    RESET = "\033[0m"
    BOLD = "\033[1m"

def log_test(test_name, status, message=""):
    symbol = f"{Colors.GREEN}âœ“{Colors.RESET}" if status == "PASS" else f"{Colors.RED}âœ—{Colors.RESET}"
    print(f"{symbol} {Colors.BOLD}{test_name}{Colors.RESET}")
    if message:
        print(f"  {message}")
    return status == "PASS"

def log_section(title):
    print(f"\n{Colors.BLUE}{'='*70}{Colors.RESET}")
    print(f"{Colors.BLUE}{Colors.BOLD}{title}{Colors.RESET}")
    print(f"{Colors.BLUE}{'='*70}{Colors.RESET}\n")

# Test results tracker
results = {
    "passed": 0,
    "failed": 0,
    "skipped": 0,
    "details": []
}

def record_result(test_name, passed, message=""):
    if passed:
        results["passed"] += 1
        results["details"].append({"test": test_name, "status": "PASS", "message": message})
    else:
        results["failed"] += 1
        results["details"].append({"test": test_name, "status": "FAIL", "message": message})

# ============================================================================
# SECTION 1: INFRASTRUCTURE TESTS
# ============================================================================

def test_infrastructure():
    log_section("SECTION 1: INFRASTRUCTURE TESTS")

    # Test 1.1: Environment Variables
    env_vars = {
        "CHROMA_HOST": os.getenv("CHROMA_HOST"),
        "CHROMA_PORT": os.getenv("CHROMA_PORT"),
        "CHROMA_COLLECTION": os.getenv("CHROMA_COLLECTION"),
        "CHROMA_CLIENT_TYPE": os.getenv("CHROMA_CLIENT_TYPE"),
        "ACTIVE_LLM_PROVIDER": os.getenv("ACTIVE_LLM_PROVIDER"),
        "CREWAI_STORAGE_DIR": os.getenv("CREWAI_STORAGE_DIR"),
    }

    all_set = all(v is not None for v in env_vars.values())
    record_result(
        "1.1 Environment Variables",
        all_set,
        f"Set: {sum(1 for v in env_vars.values() if v is not None)}/{len(env_vars)}"
    )
    log_test("1.1 Environment Variables", "PASS" if all_set else "FAIL", 
             "\n    ".join(f"{k}={v}" for k, v in env_vars.items()))

    # Test 1.2: ChromaDB Connection
    try:
        host = os.getenv("CHROMA_HOST", "chromadb")
        port = int(os.getenv("CHROMA_PORT", "8000"))
        client = chromadb.HttpClient(host=host, port=port)
        client.heartbeat()
        record_result("1.2 ChromaDB Connection", True, f"{host}:{port} reachable")
        log_test("1.2 ChromaDB Connection", "PASS", f"Connected to {host}:{port}")
    except Exception as e:
        record_result("1.2 ChromaDB Connection", False, str(e))
        log_test("1.2 ChromaDB Connection", "FAIL", f"Error: {e}")
        return  # Stop if ChromaDB not available

    # Test 1.3: ChromaDB Collection
    try:
        collection_name = os.getenv("CHROMA_COLLECTION", "ml_materials")
        col = client.get_or_create_collection(name=collection_name)
        count = col.count()
        record_result("1.3 ChromaDB Collection", True, f"{collection_name} exists with {count} docs")
        log_test("1.3 ChromaDB Collection", "PASS", 
                 f"Collection '{collection_name}' has {count} documents")
    except Exception as e:
        record_result("1.3 ChromaDB Collection", False, str(e))
        log_test("1.3 ChromaDB Collection", "FAIL", f"Error: {e}")

    # Test 1.4: CrewAI Storage Directory
    storage_dir = Path(os.getenv("CREWAI_STORAGE_DIR", "./data/crewai_memory"))
    exists = storage_dir.exists()
    record_result("1.4 CrewAI Storage Directory", exists, str(storage_dir))
    log_test("1.4 CrewAI Storage Directory", "PASS" if exists else "FAIL", 
             f"Path: {storage_dir} ({'exists' if exists else 'MISSING'})")

# ============================================================================
# SECTION 2: MCP INTEGRATION TESTS
# ============================================================================

def test_mcp_integration():
    log_section("SECTION 2: MCP INTEGRATION TESTS")

    # Test 2.1: MCP Adapter Initialization
    try:
        from src.ml_learning_assistant.crew import MLLearningAssistantCrew
        crew = MLLearningAssistantCrew()
        tools = crew._get_mcp_tools()
        tool_names = [t.name for t in tools]

        record_result("2.1 MCP Adapter Initialization", True, 
                     f"Loaded {len(tools)} tools")
        log_test("2.1 MCP Adapter Initialization", "PASS", 
                f"Tools loaded: {', '.join(tool_names)}")
    except Exception as e:
        record_result("2.1 MCP Adapter Initialization", False, str(e))
        log_test("2.1 MCP Adapter Initialization", "FAIL", f"Error: {e}")
        return

    # Test 2.2: Chroma Query Tool Availability
    chroma_tools = [t for t in tools if 'chroma' in t.name.lower()]
    has_chroma = len(chroma_tools) > 0
    record_result("2.2 Chroma Query Tool", has_chroma, 
                 f"Found {len(chroma_tools)} chroma tools")
    log_test("2.2 Chroma Query Tool", "PASS" if has_chroma else "FAIL",
             f"Chroma tools: {[t.name for t in chroma_tools]}")

    # Test 2.3: Tavily Search Tool Availability
    tavily_tools = [t for t in tools if 'tavily' in t.name.lower()]
    has_tavily = len(tavily_tools) > 0
    record_result("2.3 Tavily Search Tool", has_tavily,
                 f"Found {len(tavily_tools)} tavily tools")
    log_test("2.3 Tavily Search Tool", "PASS" if has_tavily else "FAIL",
             f"Tavily tools: {[t.name for t in tavily_tools]}")

    # Test 2.4: Execute Chroma Query Tool
    if has_chroma:
        try:
            chroma_tool = chroma_tools[0]
            result = chroma_tool._run(
                collection_name=os.getenv("CHROMA_COLLECTION", "ml_materials"),
                query_texts=["machine learning"],
                n_results=3
            )
            has_results = result and len(str(result)) > 0
            record_result("2.4 Chroma Query Execution", has_results,
                         f"Query returned {len(str(result))} chars")
            log_test("2.4 Chroma Query Execution", "PASS" if has_results else "FAIL",
                    f"Result preview: {str(result)[:200]}...")
        except Exception as e:
            record_result("2.4 Chroma Query Execution", False, str(e))
            log_test("2.4 Chroma Query Execution", "FAIL", f"Error: {e}")

    # Test 2.5: Execute Tavily Search Tool (if API key available)
    if has_tavily and os.getenv("TAVILY_API_KEY"):
        try:
            tavily_tool = tavily_tools[0]
            result = tavily_tool._run(
                query="machine learning basics",
                max_results=2
            )
            has_results = result and len(str(result)) > 0
            record_result("2.5 Tavily Search Execution", has_results,
                         f"Search returned {len(str(result))} chars")
            log_test("2.5 Tavily Search Execution", "PASS" if has_results else "FAIL",
                    f"Result preview: {str(result)[:200]}...")
        except Exception as e:
            record_result("2.5 Tavily Search Execution", False, str(e))
            log_test("2.5 Tavily Search Execution", "FAIL", f"Error: {e}")
    else:
        results["skipped"] += 1
        log_test("2.5 Tavily Search Execution", "FAIL", "TAVILY_API_KEY not set (skipped)")

# ============================================================================
# SECTION 3: AGENT TESTS
# ============================================================================

def test_agents():
    log_section("SECTION 3: AGENT TESTS")

    try:
        from src.ml_learning_assistant.crew import MLLearningAssistantCrew
        crew = MLLearningAssistantCrew()

        # Test 3.1: Researcher Agent Initialization
        try:
            researcher = crew.researcher_agent()
            has_tools = len(researcher.tools) > 0
            record_result("3.1 Researcher Agent", True,
                         f"Tools: {len(researcher.tools)}, LLM: {researcher.llm.model}")
            log_test("3.1 Researcher Agent", "PASS",
                    f"Role: {researcher.role}, Tools: {len(researcher.tools)}")
        except Exception as e:
            record_result("3.1 Researcher Agent", False, str(e))
            log_test("3.1 Researcher Agent", "FAIL", f"Error: {e}")

        # Test 3.2: Teacher Agent Initialization
        try:
            teacher = crew.teacher_agent()
            no_tools = len(teacher.tools) == 0
            record_result("3.2 Teacher Agent", True,
                         f"No tools (expected): {no_tools}")
            log_test("3.2 Teacher Agent", "PASS",
                    f"Role: {teacher.role}, Tools: {len(teacher.tools)}")
        except Exception as e:
            record_result("3.2 Teacher Agent", False, str(e))
            log_test("3.2 Teacher Agent", "FAIL", f"Error: {e}")

        # Test 3.3: Quiz Agent Initialization
        try:
            quiz = crew.quiz_agent()
            no_tools = len(quiz.tools) == 0
            record_result("3.3 Quiz Agent", True,
                         f"No tools (expected): {no_tools}")
            log_test("3.3 Quiz Agent", "PASS",
                    f"Role: {quiz.role}, Tools: {len(quiz.tools)}")
        except Exception as e:
            record_result("3.3 Quiz Agent", False, str(e))
            log_test("3.3 Quiz Agent", "FAIL", f"Error: {e}")

    except Exception as e:
        log_test("3.x Agent Tests", "FAIL", f"Failed to initialize crew: {e}")

# ============================================================================
# SECTION 4: CREW CONFIGURATION TESTS
# ============================================================================

def test_crew_configuration():
    log_section("SECTION 4: CREW CONFIGURATION TESTS")

    try:
        from src.ml_learning_assistant.crew import MLLearningAssistantCrew
        crew_instance = MLLearningAssistantCrew()

        # Test 4.1: Research Crew (with memory)
        try:
            research_crew = crew_instance.research_crew()
            has_memory = research_crew.memory
            record_result("4.1 Research Crew Config", True,
                         f"Memory: {has_memory}, Agents: {len(research_crew.agents)}")
            log_test("4.1 Research Crew Config", "PASS",
                    f"Memory enabled: {has_memory}, Agents: {len(research_crew.agents)}")
        except Exception as e:
            record_result("4.1 Research Crew Config", False, str(e))
            log_test("4.1 Research Crew Config", "FAIL", f"Error: {e}")

        # Test 4.2: Teaching Crew (no memory)
        try:
            teaching_crew = crew_instance.teaching_crew()
            no_memory = not teaching_crew.memory
            record_result("4.2 Teaching Crew Config", True,
                         f"No memory (expected): {no_memory}")
            log_test("4.2 Teaching Crew Config", "PASS",
                    f"Memory disabled: {no_memory}, Agents: {len(teaching_crew.agents)}")
        except Exception as e:
            record_result("4.2 Teaching Crew Config", False, str(e))
            log_test("4.2 Teaching Crew Config", "FAIL", f"Error: {e}")

        # Test 4.3: Quiz Research Crew (no memory)
        try:
            quiz_research_crew = crew_instance.quiz_research_crew()
            no_memory = not quiz_research_crew.memory
            record_result("4.3 Quiz Research Crew Config", True,
                         f"No memory (expected): {no_memory}")
            log_test("4.3 Quiz Research Crew Config", "PASS",
                    f"Memory disabled: {no_memory}")
        except Exception as e:
            record_result("4.3 Quiz Research Crew Config", False, str(e))
            log_test("4.3 Quiz Research Crew Config", "FAIL", f"Error: {e}")

        # Test 4.4: Quiz Crew (no memory, no tools)
        try:
            quiz_crew = crew_instance.quiz_crew()
            no_memory = not quiz_crew.memory
            record_result("4.4 Quiz Crew Config", True,
                         f"No memory (expected): {no_memory}")
            log_test("4.4 Quiz Crew Config", "PASS",
                    f"Memory disabled: {no_memory}")
        except Exception as e:
            record_result("4.4 Quiz Crew Config", False, str(e))
            log_test("4.4 Quiz Crew Config", "FAIL", f"Error: {e}")

    except Exception as e:
        log_test("4.x Crew Tests", "FAIL", f"Failed to initialize crew: {e}")

# ============================================================================
# SECTION 5: RAG (RETRIEVAL-AUGMENTED GENERATION) TESTS
# ============================================================================

def test_rag():
    log_section("SECTION 5: RAG (RETRIEVAL) TESTS")

    # Test 5.1: Direct ChromaDB Query
    try:
        host = os.getenv("CHROMA_HOST", "chromadb")
        port = int(os.getenv("CHROMA_PORT", "8000"))
        client = chromadb.HttpClient(host=host, port=port)
        collection_name = os.getenv("CHROMA_COLLECTION", "ml_materials")
        col = client.get_collection(name=collection_name)

        results_query = col.query(
            query_texts=["transformer architecture"],
            n_results=3
        )

        has_results = len(results_query.get("documents", [[]])[0]) > 0
        record_result("5.1 Direct ChromaDB Query", has_results,
                     f"Found {len(results_query.get('documents', [[]])[0])} documents")
        log_test("5.1 Direct ChromaDB Query", "PASS" if has_results else "FAIL",
                f"Query returned {len(results_query.get('documents', [[]])[0])} results")
    except Exception as e:
        record_result("5.1 Direct ChromaDB Query", False, str(e))
        log_test("5.1 Direct ChromaDB Query", "FAIL", f"Error: {e}")

    # Test 5.2: RAG via MCP Tool
    try:
        from src.ml_learning_assistant.crew import MLLearningAssistantCrew
        crew = MLLearningAssistantCrew()
        tools = crew._get_mcp_tools()
        chroma_tool = [t for t in tools if 'chroma_query' in t.name][0]

        result = chroma_tool._run(
            collection_name=collection_name,
            query_texts=["attention mechanism"],
            n_results=3
        )

        has_content = result and len(str(result)) > 50
        record_result("5.2 RAG via MCP Tool", has_content,
                     f"Retrieved {len(str(result))} chars")
        log_test("5.2 RAG via MCP Tool", "PASS" if has_content else "FAIL",
                f"Content length: {len(str(result))}")
    except Exception as e:
        record_result("5.2 RAG via MCP Tool", False, str(e))
        log_test("5.2 RAG via MCP Tool", "FAIL", f"Error: {e}")

# ============================================================================
# SECTION 6: MEMORY SYSTEM TESTS
# ============================================================================

def test_memory_system():
    log_section("SECTION 6: MEMORY SYSTEM TESTS")

    try:
        from src.ml_learning_assistant.crew import MLLearningAssistantCrew
        crew_instance = MLLearningAssistantCrew()

        storage_dir = Path(os.getenv("CREWAI_STORAGE_DIR", "./data/crewai_memory"))

        # Test 6.1: Memory Storage Directory Exists
        exists = storage_dir.exists()
        record_result("6.1 Memory Storage Directory", exists, str(storage_dir))
        log_test("6.1 Memory Storage Directory", "PASS" if exists else "FAIL",
                f"Path: {storage_dir}")

        # Test 6.2: Short-term Memory (current conversation)
        log_test("6.2 Short-term Memory", "PASS", 
                "Managed by CrewAI during conversation")
        record_result("6.2 Short-term Memory", True, "Auto-managed by CrewAI")

        # Test 6.3: Long-term Memory Storage
        try:
            memory_files = list(storage_dir.glob("*.db")) + list(storage_dir.glob("*.json"))
            has_memory = len(memory_files) > 0 or exists
            record_result("6.3 Long-term Memory Files", has_memory,
                         f"Found {len(memory_files)} memory files")
            log_test("6.3 Long-term Memory Files", 
                    "PASS" if has_memory else "WARN",
                    f"Memory files: {[f.name for f in memory_files[:5]]}")
        except Exception as e:
            record_result("6.3 Long-term Memory Files", False, str(e))
            log_test("6.3 Long-term Memory Files", "FAIL", f"Error: {e}")

    except Exception as e:
        log_test("6.x Memory Tests", "FAIL", f"Failed to initialize: {e}")

# ============================================================================
# SECTION 7: FULL PIPELINE TESTS
# ============================================================================

def test_full_pipeline():
    log_section("SECTION 7: FULL PIPELINE TESTS (E2E)")

    try:
        from src.ml_learning_assistant.crew import MLLearningAssistantCrew
        crew_instance = MLLearningAssistantCrew()

        # Test 7.1: Greeting Fast-Path (no tools)
        try:
            greeting_result = crew_instance.ask_question("Hello")
            is_greeting = len(greeting_result) < 100 and "assist" in greeting_result.lower()

            record_result("7.1 Greeting Fast-Path", is_greeting,
                         f"Response: {greeting_result}")
            log_test("7.1 Greeting Fast-Path", "PASS" if is_greeting else "FAIL",
                    f"Response: {greeting_result}")
        except Exception as e:
            record_result("7.1 Greeting Fast-Path", False, str(e))
            log_test("7.1 Greeting Fast-Path", "FAIL", f"Error: {e}")

        print(f"\n{Colors.YELLOW}âš ï¸  Skipping long-running E2E tests (Q&A, Quiz) to keep test suite fast.{Colors.RESET}")
        print(f"{Colors.YELLOW}   Run these manually after basic tests pass.{Colors.RESET}\n")
        results["skipped"] += 2

    except Exception as e:
        log_test("7.x Full Pipeline", "FAIL", f"Failed to initialize: {e}")

# ============================================================================
# MAIN TEST EXECUTION
# ============================================================================

def run_all_tests():
    print(f"{Colors.BOLD}{Colors.BLUE}")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘        ML LEARNING ASSISTANT - COMPREHENSIVE TEST SUITE           â•‘")
    print("â•‘                    TESTING EVERYTHING                              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"{Colors.RESET}\n")

    start_time = time.time()

    # Run all test sections
    test_infrastructure()
    test_mcp_integration()
    test_agents()
    test_crew_configuration()
    test_rag()
    test_memory_system()
    test_full_pipeline()

    # Final summary
    end_time = time.time()
    duration = end_time - start_time

    log_section("FINAL SUMMARY")

    total_tests = results["passed"] + results["failed"]
    pass_rate = (results["passed"] / total_tests * 100) if total_tests > 0 else 0

    print(f"{Colors.BOLD}Total Tests:{Colors.RESET} {total_tests}")
    print(f"{Colors.GREEN}âœ“ Passed:{Colors.RESET} {results['passed']}")
    print(f"{Colors.RED}âœ— Failed:{Colors.RESET} {results['failed']}")
    print(f"{Colors.YELLOW}âŠ˜ Skipped:{Colors.RESET} {results['skipped']}")
    print(f"{Colors.BOLD}Pass Rate:{Colors.RESET} {pass_rate:.1f}%")
    print(f"{Colors.BOLD}Duration:{Colors.RESET} {duration:.2f}s")

    # Save detailed results
    output_file = Path("./data/test_results") / f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text(json.dumps({
        "summary": {
            "total": total_tests,
            "passed": results["passed"],
            "failed": results["failed"],
            "skipped": results["skipped"],
            "pass_rate": pass_rate,
            "duration": duration
        },
        "details": results["details"],
        "timestamp": datetime.now().isoformat()
    }, indent=2))

    print(f"\n{Colors.BLUE}ðŸ“„ Detailed results saved to: {output_file}{Colors.RESET}\n")

    # Return exit code
    return 0 if results["failed"] == 0 else 1

if __name__ == "__main__":
    sys.exit(run_all_tests())
